# K-近鄰演算法(KNN演算法)
## 介紹
KNN演算法是一種用於分類和迴歸的無母數統計方法，由美國統計學家伊芙琳·費克斯和小約瑟夫·霍奇斯於1951年首次提出，後來由托馬斯·寇弗擴充。
## 原理
### 1.資料收集： 
收集帶有標籤的訓練數據，其中每個實例都有一個特徵向量和對應的標籤。

### 2.距離度量： 
定義特徵空間中的距離度量，通常使用歐氏距離或曼哈頓距離。這決定了如何計算兩個實例之間的相似度。

### 3.選擇K值： 
選擇要考慮的最近鄰居的數量（K值）。K值的選擇會影響模型的性能。

### 4.標籤決策：
對於要預測的新實例，計算它與訓練集中所有實例的距離，並選擇距離最近的K個鄰居。

### 5.多數決：
對於分類問題，使用多數決原則，即預測新實例的標籤為K個最近鄰居中最常見的標籤。對於回歸問題，可以計算K個鄰居的平均值作為預測值。
## 優點
### 簡單易懂：
KNN是一個直觀且易於理解的算法。它不需要複雜的模型訓練過程，並且適用於初學者。

### 無需訓練：
KNN是一種非參數化算法，因此它不需要事先訓練模型。這意味著它能夠快速適應新的數據，並且在新的數據點出現時不需要重新訓練整個模型。

### 適用於多類別問題：
KNN能夠處理多類別分類問題，並且對於標籤數量較多的情況也適用。

### 對於小型數據集效果良好：
在數據量較小的情況下，KNN的表現通常較好。因為在小數據集中，計算每個新數據點與所有訓練數據之間的距離相對較快。

### 適應非線性數據：
KNN可以處理非線性決策邊界，因此在處理複雜的數據分佈時表現較好。
## 缺點



